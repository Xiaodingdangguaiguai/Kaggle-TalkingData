{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "This notebook contains a Feature Engineering, training vs validation split and chunking the training into 10 subsets. The RAM requirements for engineering of many features are quite high therefore a lot of features are created and saved here and feature selection is done later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "sns.set()\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '~/.kaggle/competitions/talkingdata-adtracking-fraud-detection/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and downcast data types\n",
    "\n",
    "The datasets of this competition were very big and therefore downcasting data types decreases RAM requirements by a large amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    if df.loc[:, 'device'].dtype != 'int16':\n",
    "        df.loc[:, 'device'] = df.loc[:, 'device'].astype(np.int16)\n",
    "    if df.loc[:, 'os'].dtype != 'int16':\n",
    "        df.loc[:, 'os'] = df.loc[:, 'os'].astype(np.int16)\n",
    "    if df.loc[:, 'channel'].dtype != 'int16':\n",
    "        df.loc[:, 'channel'] = df.loc[:, 'channel'].astype(np.int16)\n",
    "    if 'is_attributed' in df.columns:\n",
    "        df.loc[:, 'is_attributed'] = df.loc[:, 'is_attributed'].astype(np.int8)\n",
    "    \n",
    "    if 'day' in df.columns and df.loc[:, 'day'].dtype != 'int8':\n",
    "        df.loc[:, 'day'] = df.loc[:, 'day'].astype(np.int8)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape df_train: (184903890, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83230</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:32:21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17357</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:33:34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35810</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:34:12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45745</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>478</td>\n",
       "      <td>2017-11-06 14:34:52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161007</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:35:08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip  app  device  os  channel          click_time  is_attributed\n",
       "0   83230    3       1  13      379 2017-11-06 14:32:21              0\n",
       "1   17357    3       1  19      379 2017-11-06 14:33:34              0\n",
       "2   35810    3       1  13      379 2017-11-06 14:34:12              0\n",
       "3   45745   14       1  13      478 2017-11-06 14:34:52              0\n",
       "4  161007    3       1  13      379 2017-11-06 14:35:08              0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_read = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed']\n",
    "\n",
    "df_train = pd.read_csv(path+'train.csv.zip', usecols=cols_to_read)#, nrows=6000000)\n",
    "df_train.loc[:, 'click_time'] = pd.to_datetime(df_train.click_time)\n",
    "df_train = downcast_dtypes(df_train)\n",
    "print('shape df_train: ' + str(df_train.shape))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape df_test: (18790469, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5744</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>2017-11-10 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119901</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>466</td>\n",
       "      <td>2017-11-10 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72287</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>128</td>\n",
       "      <td>2017-11-10 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78477</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>111</td>\n",
       "      <td>2017-11-10 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123080</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>328</td>\n",
       "      <td>2017-11-10 04:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip  app  device  os  channel          click_time\n",
       "0    5744    9       1   3      107 2017-11-10 04:00:00\n",
       "1  119901    9       1   3      466 2017-11-10 04:00:00\n",
       "2   72287   21       1  19      128 2017-11-10 04:00:00\n",
       "3   78477   15       1  13      111 2017-11-10 04:00:00\n",
       "4  123080   12       1  13      328 2017-11-10 04:00:00"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_read = ['ip', 'app', 'device', 'os', 'channel', 'click_time']\n",
    "\n",
    "df_test = pd.read_csv(path+'test.csv.zip', usecols=cols_to_read)#, nrows=1000000)\n",
    "df_test.loc[:, 'click_time'] = pd.to_datetime(df_test.click_time)\n",
    "df_test = downcast_dtypes(df_test)\n",
    "print('shape df_test: ' + str(df_test.shape))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape df_test_supplement: (57537505, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43570</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-09 14:23:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80528</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-09 14:23:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32323</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-09 14:25:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42887</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-09 14:26:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>119289</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>120</td>\n",
       "      <td>2017-11-09 14:26:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip  app  device  os  channel          click_time\n",
       "0   43570    3       1  18      379 2017-11-09 14:23:39\n",
       "1   80528    3       1  13      379 2017-11-09 14:23:51\n",
       "2   32323    3       1  13      379 2017-11-09 14:25:57\n",
       "3   42887    3       1  17      379 2017-11-09 14:26:03\n",
       "4  119289   58       1  30      120 2017-11-09 14:26:41"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_supplement = pd.read_csv(path+'test_supplement.csv.zip', usecols=cols_to_read)#, nrows=50000)\n",
    "df_test_supplement.loc[:, 'click_time'] = pd.to_datetime(df_test_supplement.click_time)\n",
    "df_test_supplement = downcast_dtypes(df_test_supplement)\n",
    "print('shape df_test_supplement: ' + str(df_test_supplement.shape))\n",
    "df_test_supplement.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:, 'hour'] = df_train.click_time.dt.hour.astype(np.int8)\n",
    "df_train.loc[:, 'day'] = df_train.click_time.dt.day.astype(np.int8)\n",
    "df_train = downcast_dtypes(df_train)\n",
    "\n",
    "df_test.loc[:, 'hour'] = df_test.click_time.dt.hour.astype(np.int8)\n",
    "df_test.loc[:, 'day'] = df_test.click_time.dt.day.astype(np.int8)\n",
    "df_test = downcast_dtypes(df_test)\n",
    "    \n",
    "df_test_supplement.loc[:, 'hour'] = df_test_supplement.click_time.dt.hour.astype(np.int8)\n",
    "df_test_supplement.loc[:, 'day'] = df_test_supplement.click_time.dt.day.astype(np.int8)\n",
    "df_test_supplement = downcast_dtypes(df_test_supplement)\n",
    "    \n",
    "def add_time_features(df):\n",
    "    df.loc[:, 'minute_of_day'] = df.click_time.dt.hour*60 + df.click_time.dt.minute\n",
    "    df.loc[:, 'minute_of_day'] = df.minute_of_day.astype(np.uint16)\n",
    "    \n",
    "    df.loc[:, 'timeframe_11'] = df.minute_of_day // 11\n",
    "    df.loc[:, 'timeframe_11'] = df.timeframe_11.astype(np.uint8)\n",
    "    \n",
    "    df.loc[:, 'timeframe_17'] = df.minute_of_day // 17\n",
    "    df.loc[:, 'timeframe_17'] = df.timeframe_17.astype(np.uint8)\n",
    "\n",
    "add_time_features(df_train)\n",
    "add_time_features(df_test)\n",
    "add_time_features(df_test_supplement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Given the nature of the dataset (small amount of nominal categoric columns) creating features by using grouping and aggregating different functions proved to be most useful. Useful aggregating functions were `count`, `nunique`, `var`, `mean` and `cumcount`. In addition the typically used feature `hour` is a somewhat arbitrary timeframe. Any other timeframe makes as much sense as `hour` does. Trying out other timeframes might be worth a try. In this case the tried timeframes were 11 and 17 minutes.\n",
    "\n",
    "In competitions the problem statement is slightly different from real world projects. Instead of finding a model that scores best on unseen data one has to find a model that scores best on the private subset of the test set. This allows using the test set for Feature Engineering what was absolutely necessary in this competition for obtaining a good score. Even further, one should use `test_supplement.csv` instead of `test.csv` for as many features as possible because `test.csv` is actually an incomplete subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = df_train.shape[0]\n",
    "\n",
    "df_concat = pd.concat([df_train, df_test_supplement])\n",
    "\n",
    "del df_train, df_test_supplement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy Aggregate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df, df_test, predictors):\n",
    "    \n",
    "    gby_cols = [\n",
    "                    {'gby': ['ip'],                     'agg': 'channel', 'kind': 'nunique'},\n",
    "                    {'gby': ['ip', 'day'],              'agg': 'hour',    'kind': 'nunique'},\n",
    "                    {'gby': ['ip'],                     'agg': 'app',     'kind': 'nunique'},\n",
    "                    {'gby': ['ip', 'app'],              'agg': 'os',      'kind': 'nunique'},\n",
    "                    {'gby': ['ip'],                     'agg': 'device',  'kind': 'nunique'},\n",
    "                    {'gby': ['app'],                    'agg': 'channel', 'kind': 'nunique'},\n",
    "                    {'gby': ['ip', 'device', 'os'],     'agg': 'app',     'kind': 'nunique'},\n",
    "                    {'gby': ['ip', 'day', 'hour'],      'agg': 'channel', 'kind': 'count'},\n",
    "                    {'gby': ['ip', 'app'],              'agg': 'channel', 'kind': 'count'},\n",
    "                    {'gby': ['ip', 'app', 'os'],        'agg': 'channel', 'kind': 'count'},\n",
    "                    {'gby': ['ip', 'day', 'channel'],   'agg': 'hour',    'kind': 'var'},\n",
    "                    {'gby': ['ip', 'app', 'os'],        'agg': 'hour',    'kind': 'var'},\n",
    "                    {'gby': ['ip', 'app', 'channel'],   'agg': 'day',     'kind': 'var'},\n",
    "                    {'gby': ['ip', 'app', 'channel'],   'agg': 'hour',    'kind': 'mean'},\n",
    "        \n",
    "                    {'gby': ['ip', 'day', 'timeframe_11'],   'agg': 'channel',  'kind': 'count'},\n",
    "                    {'gby': ['ip', 'day', 'timeframe_17'],   'agg': 'channel',  'kind': 'count'},\n",
    "        \n",
    "                    {'gby': ['ip', 'day', 'channel'],   'agg': 'timeframe_11',    'kind': 'var'},\n",
    "                    {'gby': ['ip', 'day', 'channel'],   'agg': 'timeframe_17',    'kind': 'var'},\n",
    "        \n",
    "                    {'gby': ['ip', 'app', 'os'],        'agg': 'timeframe_11',    'kind': 'var'},\n",
    "                    {'gby': ['ip', 'app', 'os'],        'agg': 'timeframe_17',    'kind': 'var'},\n",
    "        \n",
    "                    {'gby': ['ip', 'app', 'channel'],   'agg': 'timeframe_11',    'kind': 'mean'},\n",
    "                    {'gby': ['ip', 'app', 'channel'],   'agg': 'timeframe_17',    'kind': 'mean'},\n",
    "        \n",
    "                    {'gby': ['ip'],   'agg': 'os',    'kind': 'count'}\n",
    "                ]\n",
    "    \n",
    "    for gby_col in gby_cols:\n",
    "    \n",
    "        cols = gby_col['gby'] + [gby_col['agg']]\n",
    "        \n",
    "        colname = gby_col['agg'] + '_' + gby_col['kind'] + '_gby_' + '_'.join(gby_col['gby'])\n",
    "        predictors.append(colname)\n",
    "        \n",
    "        print('Processing ' + colname)\n",
    "        filename = 'Saves/{}.csv'.format(colname)\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            gp = pd.read_csv(filename)\n",
    "            df = df.merge(gp, on=gby_col['gby'], how='left')\n",
    "            df_test = df_test.merge(gp, on=gby_col['gby'], how='left')\n",
    "    \n",
    "        else:\n",
    "            if gby_col['kind'] == 'count':\n",
    "                gp = df[cols].groupby(by=gby_col['gby'])[gby_col['agg']].count().reset_index() \\\n",
    "                             .rename(index=str, columns={gby_col['agg']: colname})\n",
    "                df = df.merge(gp, on=gby_col['gby'], how='left')\n",
    "                df_test = df_test.merge(gp, on=gby_col['gby'], how='left')\n",
    "                \n",
    "            if gby_col['kind'] == 'mean':\n",
    "                gp = df[cols].groupby(by=gby_col['gby'])[gby_col['agg']].mean().reset_index() \\\n",
    "                             .rename(index=str, columns={gby_col['agg']: colname})\n",
    "                df = df.merge(gp, on=gby_col['gby'], how='left')\n",
    "                df_test = df_test.merge(gp, on=gby_col['gby'], how='left')\n",
    "                \n",
    "            if gby_col['kind'] == 'var':\n",
    "                gp = df[cols].groupby(by=gby_col['gby'])[gby_col['agg']].var().reset_index() \\\n",
    "                             .rename(index=str, columns={gby_col['agg']: colname})\n",
    "                df = df.merge(gp, on=gby_col['gby'], how='left')\n",
    "                df_test = df_test.merge(gp, on=gby_col['gby'], how='left')\n",
    "                \n",
    "            if gby_col['kind'] == 'nunique':\n",
    "                gp = df[cols].groupby(by=gby_col['gby'])[gby_col['agg']].nunique().reset_index() \\\n",
    "                             .rename(index=str, columns={gby_col['agg']: colname})\n",
    "                df = df.merge(gp, on=gby_col['gby'], how='left')\n",
    "                df_test = df_test.merge(gp, on=gby_col['gby'], how='left')\n",
    "\n",
    "            gp.to_csv(filename,index=False)\n",
    "        \n",
    "        if gby_col['kind'] in ['count', 'nunique']:\n",
    "            # downcast data types\n",
    "            if np.max(df.iloc[:, -1]) < 2**8/2 - 1:\n",
    "                dtype = np.int8\n",
    "            elif np.max(df.iloc[:, -1]) < 2**16/2 - 1:\n",
    "                dtype = np.int16\n",
    "            elif np.max(df.iloc[:, -1]) < 2**32/2 - 1:\n",
    "                dtype = np.int32\n",
    "            else:\n",
    "                dtype = np.int64\n",
    "            \n",
    "            df.iloc[:, -1:] = df.iloc[:, -1:].fillna(-1).astype(dtype)\n",
    "            df_test.iloc[:, -1:] = df_test.iloc[:, -1:].fillna(-1).astype(dtype)\n",
    "        \n",
    "        del gp\n",
    "        gc.collect()    \n",
    "\n",
    "    return df, df_test, predictors\n",
    "\n",
    "df_concat, df_test, predictors = add_features(df_concat, df_test, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_concat.iloc[:len_train, :]\n",
    "\n",
    "del df_concat # bye-bye df_test_supplement\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = df_train.shape[0]\n",
    "len_test = df_test.shape[0]\n",
    "\n",
    "df_concat = pd.concat([df_train, df_test]).reset_index(drop=True)\n",
    "del df_test, df_train\n",
    "gc.collect()\n",
    "\n",
    "df_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cumcount_features(df, predictors):\n",
    "    \n",
    "    gby_cols = [\n",
    "                    {'gby': ['ip', 'device', 'os'],     'agg': 'app',     'kind': 'cumcount'},\n",
    "                    {'gby': ['ip'],                     'agg': 'os',      'kind': 'cumcount'}\n",
    "               ]\n",
    "    \n",
    "    for gby_col in gby_cols:\n",
    "    \n",
    "        cols = gby_col['gby'] + [gby_col['agg']]\n",
    "        \n",
    "        colname = gby_col['agg'] + '_' + gby_col['kind'] + '_gby_' + '_'.join(gby_col['gby'])\n",
    "        predictors.append(colname)\n",
    "        \n",
    "        print('Processing ' + colname)\n",
    "        filename='Saves/{}.csv'.format(colname)\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            gp = pd.read_csv(filename,header=None)\n",
    "            df[colname] = gp\n",
    "            \n",
    "            del gp\n",
    "            gc.collect()  \n",
    "        else:\n",
    "            gp = df[cols].groupby(by=gby_col['gby'])[gby_col['agg']].cumcount()\n",
    "            df[colname] = gp.values.astype(np.int32)\n",
    "\n",
    "    return df, predictors\n",
    "\n",
    "df_concat, predictors = add_cumcount_features(df_concat, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next click features\n",
    "\n",
    "Next click features have proven to be one of the most important features for this dataset as proven by various kernels. Therefore these will be added here aswell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_next_click_features(df, predictors):\n",
    "    print('Processing next_click')\n",
    "    \n",
    "    new_feature = 'next_click'\n",
    "    filename='Saves/next_click.csv'\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        print('loading from save file')\n",
    "        df_temp = pd.read_csv(filename)\n",
    "        QQ = df_temp.QQ\n",
    "        df.loc[:, 'category'] = df_temp.category\n",
    "        del df_temp\n",
    "    else:\n",
    "        D=2**26\n",
    "        df['category'] = (df['ip'].astype(str) + \"_\" + df['app'].astype(str) + \"_\" + df['device'].astype(str) \\\n",
    "                         + \"_\" + df['os'].astype(str)).apply(hash) % D\n",
    "        click_buffer= np.full(D, 3000000000, dtype=np.uint32)\n",
    "\n",
    "        df['epochtime'] = df['click_time'].astype(np.int64) // 10**9\n",
    "        next_clicks= []\n",
    "        for category, t in zip(reversed(df['category'].values), reversed(df['epochtime'].values)):\n",
    "            next_clicks.append(click_buffer[category]-t)\n",
    "            click_buffer[category]= t\n",
    "            \n",
    "        del(click_buffer)\n",
    "        QQ= list(reversed(next_clicks))\n",
    "\n",
    "        print('saving')\n",
    "        df_temp = pd.DataFrame()\n",
    "        df_temp.loc[:, 'QQ'] = QQ\n",
    "        df_temp.loc[:, 'category'] = df.category\n",
    "        df_temp.to_csv(filename,index=False)\n",
    "        del df_temp\n",
    "\n",
    "    df[new_feature] = QQ\n",
    "    df[new_feature] = df[new_feature].astype(np.int32)\n",
    "    predictors.append(new_feature)\n",
    "\n",
    "    df[new_feature+'_shift'] = pd.DataFrame(QQ).astype(np.float32).shift(+1).values\n",
    "    predictors.append(new_feature+'_shift')\n",
    "    \n",
    "    del QQ\n",
    "    gc.collect()\n",
    "    \n",
    "    return df, predictors\n",
    "\n",
    "df_concat, predictors = add_next_click_features(df_concat, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the time to next click for each group\n",
    "for spec in [{'groupby': ['ip']}, {'groupby': ['ip', 'app']}, {'groupby': ['ip', 'channel']}, \n",
    "             {'groupby': ['ip', 'os']}]:\n",
    "    \n",
    "    # Name of new feature\n",
    "    new_feature = '{}_nextClick'.format('_'.join(spec['groupby']))    \n",
    "    \n",
    "    # Unique list of features to select\n",
    "    all_features = spec['groupby'] + ['click_time']\n",
    "    \n",
    "    # Run calculation\n",
    "    print(\">> Grouping by {}, and saving time to next click in: {}\".format(spec['groupby'], new_feature))\n",
    "    gb = df_concat[all_features].groupby(spec['groupby']).click_time \\\n",
    "                                .transform(lambda x: x.diff().shift(-1)).dt.seconds.astype(np.float16)\n",
    "    df_concat[new_feature] = df_concat[all_features].groupby(spec['groupby']).click_time \\\n",
    "                                 .transform(lambda x: x.diff().shift(-1)).dt.seconds.astype(np.float16)\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_concat.iloc[:len_train, :]\n",
    "print(df_train.shape)\n",
    "\n",
    "df_test = df_concat.iloc[len_train:, :]\n",
    "print(df_test.shape)\n",
    "\n",
    "del df_concat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:, 'category_mod16'] = df_train.category % 2**16\n",
    "df_train.loc[:, 'category_mod16'] = df_train.loc[:, 'category_mod16'].astype(np.int32)\n",
    "df_train = df_train.drop('epochtime', axis=1)\n",
    "\n",
    "df_valid.loc[:, 'category_mod16'] = df_valid.category % 2**16\n",
    "df_valid.loc[:, 'category_mod16'] = df_valid.loc[:, 'category_mod16'].astype(np.int32)\n",
    "df_valid = df_valid.drop('epochtime', axis=1)\n",
    "\n",
    "df_test.loc[:, 'category_mod16'] = df_test.category % 2**16\n",
    "df_test.loc[:, 'category_mod16'] = df_test.loc[:, 'category_mod16'].astype(np.int32)\n",
    "df_test = df_test.drop('epochtime', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / validation split \n",
    "\n",
    "As proposed and reasoned in `ExploratoryDataAnalysis.ipynb` the training and validation split will be a time-based split (based on the private subset of the test set) as well as a selection of the occuring hours in the private subset of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "split_date = pd.Timestamp(year=2017, month=11, day=9, hour=4)\n",
    "\n",
    "df_train = downcast_dtypes(df_train)\n",
    "\n",
    "srs_mask = df_train.click_time < split_date\n",
    "\n",
    "df_valid = df_train.loc[~srs_mask, :]\n",
    "df_train = df_train.loc[srs_mask, :]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train_val, df_valid = train_test_split(df_valid, test_size=5000000/df_valid.shape[0])\n",
    "\n",
    "del srs_mask\n",
    "gc.collect()\n",
    "\n",
    "print('train shape: ' + str(df_train.shape))\n",
    "print('train_val shape: ' + str(df_train_val.shape))\n",
    "print('valid shape: ' + str(df_valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train, df_train_val])\n",
    "\n",
    "del df_train_val\n",
    "gc.collect()\n",
    "\n",
    "print('train shape: ' + str(df_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downcast binary target column \n",
    "df_train.loc[:, 'is_attributed'] = df_train.loc[:, 'is_attributed'].astype(np.int8)\n",
    "df_valid.loc[:, 'is_attributed'] = df_valid.loc[:, 'is_attributed'].astype(np.int8)\n",
    "df_test.loc[:, 'is_attributed'] = df_test.loc[:, 'is_attributed'].fillna(0).astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking and target mean encodings\n",
    "\n",
    "The training data will be subsampled in 10 random chunks for faster loading, feature selection and hyperparameter tuning. In addition target mean encodings are processed. To avoid overfitting those cross validation is used. Due to the large target imbalance in the dataset the used cross validation is stratified.\n",
    "\n",
    "The data is saved in HDF5 files to allow faster data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_valid.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = downcast_dtypes(df_train)\n",
    "df_valid = downcast_dtypes(df_valid)\n",
    "df_test = downcast_dtypes(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cols_to_encode = ['app', 'device', 'os', 'channel', 'hour', 'timeframe_11', 'timeframe_17', 'category_mod16']\n",
    "n_splits=10\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "\n",
    "with pd.HDFStore('store_enc_chunks.h5',  mode='w') as store:\n",
    "    \n",
    "    for i, (train_index, valid_index) in enumerate(skf.split(df_train, df_train.is_attributed)):\n",
    "        print('Starting iteration ' + str(i))\n",
    "        \n",
    "        # target mean encodings\n",
    "        for col in cols_to_encode:\n",
    "            print('\\t Processing mean encoding for ' + col)\n",
    "            df_train_gby_col = df_train.loc[train_index, [col, 'is_attributed']].groupby(col) \\\n",
    "                                   .is_attributed.mean()\n",
    "            df_train.loc[valid_index, 'target_mean_' + col] = df_train.loc[valid_index, col].map(df_train_gby_col)\n",
    "            df_train.loc[:, 'target_mean_' + col] = df_train.loc[:, 'target_mean_' + col].astype(np.float16)\n",
    "\n",
    "            if i == 0:\n",
    "                df_valid.loc[:, 'target_mean_' + col] = (df_valid.loc[:, col].map(df_train_gby_col) / n_splits)\n",
    "                df_valid.loc[:, 'target_mean_' + col] = df_valid.loc[:, 'target_mean_' + col].astype(np.float16)\n",
    "                \n",
    "                df_test.loc[:, 'target_mean_' + col] = (df_test.loc[:, col].map(df_train_gby_col) / n_splits)\n",
    "                df_test.loc[:, 'target_mean_' + col] = df_test.loc[:, 'target_mean_' + col].astype(np.float16)\n",
    "            else:\n",
    "                df_valid.loc[:, 'target_mean_' + col] += (df_valid.loc[:, col].map(df_train_gby_col) / n_splits)\n",
    "                df_valid.loc[:, 'target_mean_' + col] = df_valid.loc[:, 'target_mean_' + col].astype(np.float16)\n",
    "                \n",
    "                df_test.loc[:, 'target_mean_' + col] += (df_test.loc[:, col].map(df_train_gby_col) / n_splits)\n",
    "                df_test.loc[:, 'target_mean_' + col] = df_test.loc[:, 'target_mean_' + col].astype(np.float16)\n",
    "\n",
    "        del df_train_gby_col\n",
    "        gc.collect()\n",
    "\n",
    "        # frequency features\n",
    "        for col in ['ip', 'app', 'device', 'os', 'channel']:\n",
    "            print('\\t Processing frequency encoding for column ' + col)\n",
    "\n",
    "            # Get counts, sums and frequency of is_attributed\n",
    "            df_train_freq = pd.DataFrame({\n",
    "                    'sums': df_train.loc[train_index, :].groupby(col)['is_attributed'].sum(),\n",
    "                    'counts': df_train.loc[train_index, :].groupby(col)['is_attributed'].count()\n",
    "            })\n",
    "            df_train_freq.loc[:, 'freq'] = (df_train_freq.sums / df_train_freq.counts)\n",
    "            df_train_freq.loc[:, 'freq'] = df_train_freq.loc[:, 'freq'].astype(np.float16)\n",
    "\n",
    "            # If we have less than 3 observations, e.g. for an IP, then assume freq of 0\n",
    "            df_train_freq.loc[df_train_freq.counts <= 3, 'freq'] = 0        \n",
    "\n",
    "            df_train.loc[valid_index, col+'_freq'] = df_train.loc[valid_index, col].map(df_train_freq['freq'])\n",
    "            df_train.loc[:, col+'_freq'] = df_train.loc[:, col+'_freq'].astype(np.float16)\n",
    "            \n",
    "            if i == 0:\n",
    "                df_valid.loc[:, col+'_freq'] = (df_valid.loc[:, col].map(df_train_freq['freq']) / n_splits)\n",
    "                df_valid.loc[:, col+'_freq'] = df_valid.loc[:, col+'_freq'].astype(np.float16)\n",
    "                \n",
    "                df_test.loc[:, col+'_freq'] = (df_test.loc[:, col].map(df_train_freq['freq']) / n_splits)\n",
    "                df_test.loc[:, col+'_freq'] = df_test.loc[:, col+'_freq'].astype(np.float16)\n",
    "            else:\n",
    "                df_valid.loc[:, col+'_freq'] += (df_valid.loc[:, col].map(df_train_freq['freq']) / n_splits)\n",
    "                df_valid.loc[:, col+'_freq'] = df_valid.loc[:, col+'_freq'].astype(np.float16)\n",
    "                \n",
    "                df_test.loc[:, col+'_freq'] += (df_test.loc[:, col].map(df_train_freq['freq']) / n_splits)\n",
    "                df_test.loc[:, col+'_freq'] = df_test.loc[:, col+'_freq'].astype(np.float16)\n",
    "                \n",
    "        del df_train_freq\n",
    "        gc.collect()\n",
    "        \n",
    "        df_train = downcast_dtypes(df_train)\n",
    "        store.append('df_train_chunk' + str(i), df_train.loc[valid_index, :], \n",
    "                     data_columns=df_train.columns, format='table')\n",
    "        \n",
    "    df_valid = downcast_dtypes(df_valid)\n",
    "    store.append('df_valid', df_valid, data_columns=df_valid.columns, format='table')\n",
    "    \n",
    "    df_test = downcast_dtypes(df_test)\n",
    "    store.append('df_test', df_test, data_columns=df_test.columns, format='table')\n",
    "\n",
    "df_train = df_train.drop(cols_to_encode, axis=1)\n",
    "df_train = downcast_dtypes(df_train)\n",
    "\n",
    "df_test = df_test.drop(cols_to_encode, axis=1)\n",
    "df_test = downcast_dtypes(df_test)\n",
    "\n",
    "df_train.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['app_nunique_gby_ip', 'app_nunique_gby_ip_device_os',\n",
       "       'channel_count_gby_ip_app', 'channel_count_gby_ip_app_os',\n",
       "       'channel_count_gby_ip_day_hour',\n",
       "       'channel_count_gby_ip_day_timeframe_11',\n",
       "       'channel_count_gby_ip_day_timeframe_17', 'channel_nunique_gby_app',\n",
       "       'channel_nunique_gby_ip', 'click_time', 'day',\n",
       "       'day_var_gby_ip_app_channel', 'device_nunique_gby_ip',\n",
       "       'hour_mean_gby_ip_app_channel', 'hour_nunique_gby_ip_day',\n",
       "       'hour_var_gby_ip_app_os', 'hour_var_gby_ip_day_channel', 'ip',\n",
       "       'is_attributed', 'minute_of_day', 'os_count_gby_ip',\n",
       "       'os_nunique_gby_ip_app', 'timeframe_11_mean_gby_ip_app_channel',\n",
       "       'timeframe_11_var_gby_ip_app_os', 'timeframe_11_var_gby_ip_day_channel',\n",
       "       'timeframe_17_mean_gby_ip_app_channel',\n",
       "       'timeframe_17_var_gby_ip_app_os', 'timeframe_17_var_gby_ip_day_channel',\n",
       "       'app_cumcount_gby_ip_device_os', 'os_cumcount_gby_ip', 'category',\n",
       "       'next_click', 'next_click_shift', 'ip_nextClick', 'ip_app_nextClick',\n",
       "       'ip_channel_nextClick', 'ip_os_nextClick', 'target_mean_app',\n",
       "       'target_mean_device', 'target_mean_os', 'target_mean_channel',\n",
       "       'target_mean_hour', 'target_mean_timeframe_11',\n",
       "       'target_mean_timeframe_17', 'target_mean_category_mod16', 'ip_freq',\n",
       "       'app_freq', 'device_freq', 'os_freq', 'channel_freq'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
